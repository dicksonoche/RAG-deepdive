{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchai_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16513df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.ui.workspace import CloudWorkspace\n",
    "\n",
    "from evidently import Dataset, DataDefinition, Report\n",
    "from evidently.descriptors import *\n",
    "from evidently.presets import TextEvals\n",
    "from evidently.llm.templates import BinaryClassificationPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dabea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "OA_client = OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e632123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = CloudWorkspace(url=\"https://app.evidently.cloud/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa1fc3e",
   "metadata": {},
   "source": [
    "Load testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb7133",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"0198e80b-dd15-7928-a1ac-54ae33419ecc\"\n",
    "testing_dataset = client.load_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c4002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff4bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset.as_dataframe()[[\"question\", \"answers\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a476a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative way - write some questions and answers yourself\n",
    "my_questions = [\n",
    "    \"How do I do this?\",\n",
    "    \"How do I do that?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8b9c4",
   "metadata": {},
   "source": [
    "Simulate RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a88877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types import vector_store\n",
    "\n",
    "\n",
    "def load_and_index_from(url):\n",
    "    # Step 1: Load file content from GitHub raw URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status() # Raise error if download fails\n",
    "    content = response.text\n",
    "    \n",
    "    # Split into chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=400)\n",
    "    documents = [Document(page_content=content)]\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Generate embeddings and create FAISS index\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771a78ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the index for relevant information\n",
    "def search_documents(query, vector_store):\n",
    "    search_results = vector_store.similarity_search(query, k=5)\n",
    "    context = \"\\n\".join([doc.page_content for doc in search_results])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3750bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question, context, model=\"gpt-4o-mini\"):\n",
    "    response = OA_client.responses.create(\n",
    "        instructions=\"Your task is to answer the provided question based on the context.\",\n",
    "        model=model,\n",
    "        input=f\"The retrieved context is {context} \\n {question}\"\n",
    "    )\n",
    "\n",
    "    text = response.output_text if response else None\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42414012",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://docs.evidentlyai.com/llms-full.txt\"\n",
    "vector_store = load_and_index_from(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1573fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = testing_dataset.as_dataframe().questions.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f4c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = [search_documents(question, vector_store) for question in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d363fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_answers = [\n",
    "    generate_response(question, context)\n",
    "    for question, context in zip(questions, contexts)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d9750",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c4fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ec3e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_frame = pd.DataFrame()\n",
    "testing_frame['question'] = questions\n",
    "testing_frame['reference_answer'] = testing_dataset.as_dataframe().answers.values\n",
    "testing_frame['generated_answer'] = generated_answers\n",
    "testing_frame['context'] = contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_frame[[\"question\", \"generated_answer\", \"reference_answer\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf32d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "contradiction_check = BinaryClassificationPromptTemplate(\n",
    "    criteria = \"\"\"Label an ANSWER as **contradictory** only if it directly contradicts any part of that REFERENCE.\n",
    "    Differences in length or wording are acceptable. It is also acceptable if the ANSWER adds new details or but not acceptable if the ANSWER omits information, as long as **it is a fact and not contradictory**\n",
    "    Your task is to compare factual consistency only - not completeness, relevance, or style.\n",
    "    \n",
    "    REFERENCE:\n",
    "    =====\n",
    "    {reference}\n",
    "    =====\n",
    "    \"\"\",\n",
    "    target_category = \"contradictory\",\n",
    "    non_target_category = \"non-contradictory\",\n",
    "    uncertainty = \"unknown\",\n",
    "    include_reasoning = True,\n",
    "    pre_messages = [(\"system\", \"You are an expert evaluator. You will be given an ANSWER and a REFERENCE.\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed310e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = Dataset.from_pandas(\n",
    "    testing_frame,\n",
    "    data_definition=DataDefinition(),\n",
    "    descriptors=[\n",
    "        Sentiment(\"generated_answer\"),\n",
    "        TextLength(\"generated_answer\"),\n",
    "        FaithfulnessLLMEval(\"generated_answer\", context=\"context\"),\n",
    "        LLMEval(\"generated_answer\", template=contradiction_check, additional_columns={\"reference_answer\": \"reference\"},\n",
    "                provider = \"openai\", model = \"gpt-4o-mini\", alias=\"Contradictions\"),\n",
    "    ]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset.as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211adbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report([\n",
    "    TextEvals()\n",
    "])\n",
    "\n",
    "my_eval = report.run(testing_dataset, None)\n",
    "my_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e98f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROJECT_ID = \"0198e7f5-2a11-7b58-b708-d55042be6279\"\n",
    "#client.add_run(PROJECT_ID, my_eval, include_data=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
